{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6c1227a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# CELL 1 — imports & setup\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Text / NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# download nltk stopwords first-run\n",
    "nltk.download('stopwords')\n",
    "\n",
    "DATA_PATH = Path(\"../data/raw/raw_analyst_ratings.csv\")  # your file\n",
    "OUT_DIR = Path(\"../data/processed/eda_outputs\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pd.set_option('display.max_columns', 120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593eb830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ba9fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2 — robust CSV loader (handles very large files)\n",
    "def load_csv_memory_friendly(path, sample_frac=None, nrows=None, chunksize=200_000):\n",
    "    \"\"\"\n",
    "    - If sample_frac provided, returns a random sample (read in chunks).\n",
    "    - If nrows provided, reads only first nrows.\n",
    "    - Otherwise attempts fast full read.\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "    if nrows is not None:\n",
    "        df = pd.read_csv(path, nrows=nrows)\n",
    "        return df\n",
    "\n",
    "    # quick attempt to read full file (fast path)\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        return df\n",
    "    except MemoryError:\n",
    "        # fallback: sample rows streaming\n",
    "        if sample_frac is None:\n",
    "            raise MemoryError(\"File too large to load in memory; provide sample_frac.\")\n",
    "        rng = np.random.default_rng(0)\n",
    "        chunks = []\n",
    "        for chunk in pd.read_csv(path, chunksize=chunksize):\n",
    "            mask = rng.random(chunk.shape[0]) < sample_frac\n",
    "            if mask.any():\n",
    "                chunks.append(chunk.loc[mask])\n",
    "        if chunks:\n",
    "            return pd.concat(chunks, ignore_index=True)\n",
    "        else:\n",
    "            return pd.DataFrame(columns=chunk.columns)\n",
    "\n",
    "# Use it:\n",
    "df = load_csv_memory_friendly(DATA_PATH, sample_frac=None, nrows=None)\n",
    "print(\"Loaded rows:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a678f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3 — inspect columns and sample rows\n",
    "print(df.columns.tolist())\n",
    "display(df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf88961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4 — normalize column names and basic cleanup\n",
    "# Based on your sample header: headline,url,publisher,date,stock\n",
    "expected = ['headline','url','publisher','date','stock']\n",
    "# normalize columns (lowercase, strip)\n",
    "df.columns = [c.strip().lower() for c in df.columns]\n",
    "\n",
    "# Keep only expected columns if extras exist\n",
    "keep = [c for c in expected if c in df.columns]\n",
    "df = df[keep].copy()\n",
    "\n",
    "# drop exact duplicates (same url or same headline + date)\n",
    "if 'url' in df.columns:\n",
    "    df = df.drop_duplicates(subset=['url'])\n",
    "else:\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "# Basic info\n",
    "print(\"Rows after dedup:\", len(df))\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721c7825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5 — parse dates with timezone handling\n",
    "# Your sample date looks like: 2020--05 10:30:54-04:00  (typo??) — we'll try flexible parsing\n",
    "def safe_parse_datetime(s):\n",
    "    try:\n",
    "        return pd.to_datetime(s, utc=True, errors='coerce')\n",
    "    except Exception:\n",
    "        return pd.to_datetime(s, errors='coerce')\n",
    "\n",
    "df['date_parsed'] = df['date'].astype(str).apply(safe_parse_datetime)\n",
    "# If parsing produced naive timestamps, make them UTC-aware; assume input already has offset per description.\n",
    "# Report parsing issues\n",
    "n_bad = df['date_parsed'].isna().sum()\n",
    "print(f\"Failed to parse {n_bad} date rows out of {len(df)}\")\n",
    "# If many fails, show some examples\n",
    "if n_bad:\n",
    "    display(df.loc[df['date_parsed'].isna(), ['date']].head(10))\n",
    "\n",
    "# create common columns for analysis\n",
    "df['date_utc'] = df['date_parsed'].dt.tz_convert('UTC')   # all to UTC\n",
    "df['date_local'] = df['date_parsed']                     # original tz if present\n",
    "df['date'] = df['date_utc'].dt.normalize()               # trading-day date (midnight UTC)\n",
    "df['year'] = df['date_utc'].dt.year\n",
    "df['month'] = df['date_utc'].dt.month\n",
    "df['day'] = df['date_utc'].dt.day\n",
    "df['hour'] = df['date_utc'].dt.hour\n",
    "df['weekday'] = df['date_utc'].dt.day_name()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfbf734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6 — Descriptive statistics: headline lengths & NAs\n",
    "df['headline'] = df['headline'].astype(str)\n",
    "df['headline_len'] = df['headline'].str.len().fillna(0).astype(int)\n",
    "\n",
    "print(\"Headline length stats:\\n\", df['headline_len'].describe())\n",
    "\n",
    "# Nulls per column\n",
    "null_summary = df.isna().sum().to_frame('n_nulls')\n",
    "null_summary['pct_null'] = null_summary['n_nulls'] / len(df) * 100\n",
    "display(null_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bd31c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot headline length distribution\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(df['headline_len'], bins=50)\n",
    "plt.title(\"Headline length distribution\")\n",
    "plt.xlabel(\"length (chars)\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR/\"headline_length_hist.png\", dpi=150)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81450735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7 — Publisher counts (top publishers)\n",
    "publisher_counts = df['publisher'].value_counts(dropna=True)\n",
    "display(publisher_counts.head(30))\n",
    "\n",
    "plt.figure(figsize=(8,10))\n",
    "publisher_counts.head(30).sort_values().plot(kind='barh')\n",
    "plt.title(\"Top 30 publishers by article count\")\n",
    "plt.xlabel(\"count\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR/\"top_publishers.png\", dpi=150)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980ac747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8 — Publication trends over time (articles per day)\n",
    "daily_counts = df.groupby('date').size().rename('n_articles').reset_index()\n",
    "daily_counts = daily_counts.sort_values('date')\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(daily_counts['date'], daily_counts['n_articles'])\n",
    "plt.title(\"Articles per day (time series)\")\n",
    "plt.xlabel(\"date\")\n",
    "plt.ylabel(\"n_articles\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR/\"articles_per_day.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# show peaks\n",
    "daily_counts.nlargest(10, 'n_articles')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0de92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9 — Time-of-day and weekday distributions\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.countplot(x='hour', data=df, order=range(0,24))\n",
    "plt.title(\"Articles by hour of day (UTC)\")\n",
    "plt.savefig(OUT_DIR/\"articles_by_hour.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "order = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n",
    "sns.countplot(x='weekday', data=df, order=order)\n",
    "plt.title(\"Articles by weekday\")\n",
    "plt.savefig(OUT_DIR/\"articles_by_weekday.png\", dpi=150)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826491ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10 — Text analysis: cleaning & tokens\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "WORD_RE = re.compile(r'\\b[a-z]{2,}\\b')\n",
    "\n",
    "def tokens(text):\n",
    "    if not isinstance(text, str): return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', ' ', text)  # remove urls\n",
    "    toks = WORD_RE.findall(text)\n",
    "    toks = [t for t in toks if t not in STOPWORDS]\n",
    "    return toks\n",
    "\n",
    "# build token frequency (sample to speed up if large)\n",
    "SAMPLE_N = 200_000\n",
    "iter_df = df['headline']\n",
    "if len(df) > SAMPLE_N:\n",
    "    iter_df = df['headline'].sample(SAMPLE_N, random_state=0)\n",
    "\n",
    "all_tokens = []\n",
    "for t in tqdm(iter_df, desc=\"tokenizing\"):\n",
    "    all_tokens.extend(tokens(t))\n",
    "\n",
    "most_common = Counter(all_tokens).most_common(60)\n",
    "print(\"Top words:\", most_common[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb021db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 11 — Bigrams (frequent phrases)\n",
    "from nltk import ngrams\n",
    "\n",
    "def top_bigrams(headlines, top_k=40):\n",
    "    c = Counter()\n",
    "    for h in headlines:\n",
    "        ts = tokens(h)\n",
    "        for bg in ngrams(ts, 2):\n",
    "            c[' '.join(bg)] += 1\n",
    "    return c.most_common(top_k)\n",
    "\n",
    "bigrams_top = top_bigrams(iter_df, top_k=50)\n",
    "print(\"Top bigrams:\", bigrams_top[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37419412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 12 — Lightweight topic modeling (TF-IDF + KMeans)\n",
    "# We'll vectorize headlines and cluster them. Adjust n_clusters as needed.\n",
    "SAMPLE_TEXTS = df['headline'].dropna().sample(min(20000, len(df)), random_state=0).tolist()\n",
    "\n",
    "tfv = TfidfVectorizer(max_df=0.8, min_df=10, ngram_range=(1,2), max_features=5000)\n",
    "X = tfv.fit_transform(SAMPLE_TEXTS)\n",
    "\n",
    "n_clusters = 12\n",
    "km = KMeans(n_clusters=n_clusters, random_state=0, n_init=10)\n",
    "km.fit(X)\n",
    "labels = km.labels_\n",
    "\n",
    "# top terms per cluster\n",
    "terms = tfv.get_feature_names_out()\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "cluster_terms = {}\n",
    "for i in range(n_clusters):\n",
    "    cluster_terms[i] = [terms[ind] for ind in order_centroids[i, :20]]\n",
    "    print(f\"Cluster {i} top terms:\", cluster_terms[i][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd70b629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13 — Save processed data & summary CSVs for later work\n",
    "df.to_parquet(OUT_DIR/\"news_processed.parquet\", index=False)\n",
    "pd.DataFrame(most_common, columns=['word','count']).to_csv(OUT_DIR/\"top_words.csv\", index=False)\n",
    "pd.DataFrame(bigrams_top, columns=['bigram','count']).to_csv(OUT_DIR/\"top_bigrams.csv\", index=False)\n",
    "daily_counts.to_csv(OUT_DIR/\"daily_counts.csv\", index=False)\n",
    "publisher_counts.to_csv(OUT_DIR/\"publisher_counts.csv\", header=['count'])\n",
    "print(\"Saved outputs to\", OUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
